1.	Hybrid Optimization Strategy (SWATS)
SWATS paper (Keskar et al., 2017): Improving Generalization Performance by Switching from Adam to SGD
•	rapid convergence (Adam) + optimal generalization (SGD)
•	switching at defined step with defined learning rate based on Adam optimiser.
•	Compare with Adam, AdamW, SGD, Adam_SGD hand switch

2.	Domain Adversarial Training via Gradient Reversal
Ganin paper: Unsupervised Domain Adaptation by Backpropagation
•	Domain Classifier + Gradient Reversal Layer (GRL)
•	Minimize the CTC loss to predict the correct text.
•	Maximize the error of the domain classifier, ensuring contains no discriminative information about the recording day.
